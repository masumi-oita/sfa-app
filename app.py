import streamlit as st
import pandas as pd
import plotly.express as px
import json
from datetime import datetime, timedelta, date
from google.cloud import bigquery
from google.oauth2 import service_account
from google.api_core.exceptions import BadRequest, Forbidden

# =========================
# CONFIG
# =========================
BQ_PROJECT = "salesdb-479915"
TABLE_SNAPSHOT_2Y = f"{BQ_PROJECT}.sales_data.sales_history_2year"
TABLE_INC = f"{BQ_PROJECT}.sales_data.sales_details_snapshots"
LOOKBACK_DAYS_NEW = 365

st.set_page_config(page_title="Strategic Sales Console", layout="wide")

# =========================
# BQ client
# =========================
def get_bq_client():
    key_dict = json.loads(st.secrets["gcp_service_account"]["json_key"])
    credentials = service_account.Credentials.from_service_account_info(key_dict)
    return bigquery.Client(credentials=credentials, project=key_dict["project_id"])

# =========================
# date helpers
# =========================
def fy_year(d: date) -> int:
    return d.year if d.month >= 4 else d.year - 1

def fy_start(d: date) -> date:
    return date(d.year, 4, 1) if d.month >= 4 else date(d.year - 1, 4, 1)

def same_day_last_year(d: date) -> date:
    try:
        return date(d.year - 1, d.month, d.day)
    except ValueError:
        return date(d.year - 1, d.month, 28)

def month_start(d: date) -> date:
    return d.replace(day=1)

def yen(x) -> str:
    try:
        return f"Â¥{float(x):,.0f}"
    except Exception:
        return ""

def safe_parse_date_series(s: pd.Series) -> pd.Series:
    s = s.astype(str).str.strip()
    d1 = pd.to_datetime(s, format="%Y%m%d", errors="coerce")
    d2 = pd.to_datetime(s, errors="coerce")
    return d1.fillna(d2)

# =========================
# schema helpers
# =========================
@st.cache_data(ttl=3600)
def get_table_columns(table_fqn: str) -> list[str]:
    """
    table_fqn: project.dataset.table
    """
    client = get_bq_client()
    try:
        t = client.get_table(table_fqn)
        return [f.name for f in t.schema]
    except Exception as e:
        st.error(f"ãƒ†ãƒ¼ãƒ–ãƒ«ã‚¹ã‚­ãƒ¼ãƒå–å¾—ã«å¤±æ•—: {table_fqn}")
        st.write(str(e))
        st.stop()

def select_existing(cols_wanted: list[str], existing: list[str]) -> list[str]:
    return [c for c in cols_wanted if c in existing]

def run_query_df(sql: str, label: str):
    client = get_bq_client()
    try:
        return client.query(sql).to_dataframe(create_bqstorage_client=False)
    except BadRequest as e:
        st.error(f"[{label}] BigQuery BadRequest")
        # ã“ã“ãŒèµ¤å¡—ã‚Šã•ã‚Œãªã„ç¯„å›²ã§â€œã§ãã‚‹ã ã‘â€å‡ºã™
        st.write("SQL:", sql)
        st.write("Error:", str(e))
        st.stop()
    except Forbidden as e:
        st.error(f"[{label}] BigQuery Forbiddenï¼ˆæ¨©é™ä¸è¶³ã®å¯èƒ½æ€§ï¼‰")
        st.write("Error:", str(e))
        st.stop()
    except Exception as e:
        st.error(f"[{label}] BigQuery query failed")
        st.write("SQL:", sql)
        st.write("Error:", str(e))
        st.stop()

# =========================
# LOADERS (schema-safe)
# =========================
@st.cache_data(ttl=1800)
def load_snapshot_2y():
    # ã‚¹ã‚­ãƒ¼ãƒå–å¾—ï¼ˆproject.dataset.tableå½¢å¼ï¼‰
    cols = get_table_columns(TABLE_SNAPSHOT_2Y)

    wanted = [
        "å¾—æ„å…ˆã‚³ãƒ¼ãƒ‰", "å¾—æ„å…ˆå",
        "å•†å“ã‚³ãƒ¼ãƒ‰", "å•†å“å", "åŒ…è£…å˜ä½",
        "ãƒ­ãƒƒãƒˆNo", "ä½¿ç”¨æœŸé™",
        "æ•°é‡", "å˜ä¾¡",
        "åˆè¨ˆé‡‘é¡", "ç²—åˆ©",
        "JANã‚³ãƒ¼ãƒ‰", "YJã‚³ãƒ¼ãƒ‰",
        "ãƒ¦ãƒ‹ãƒ¼ã‚¯ã‚³ãƒ¼ãƒ‰_YJ",
        "è²©å£²æ—¥",
    ]
    use_cols = select_existing(wanted, cols)
    if not use_cols:
        st.error("[SNAPSHOT] å–å¾—ã§ãã‚‹åˆ—ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
        st.write("ãƒ†ãƒ¼ãƒ–ãƒ«åˆ—:", cols)
        st.stop()

    select_sql = ",\n      ".join([f"`{c}`" for c in use_cols])
    q = f"""
    SELECT
      {select_sql}
    FROM `{TABLE_SNAPSHOT_2Y}`
    """

    df = run_query_df(q, "SNAPSHOT")

    # å¿…é ˆåˆ—ã ã‘ã¯å³å¯†ã«ç¢ºèª
    required = ["å¾—æ„å…ˆã‚³ãƒ¼ãƒ‰","å¾—æ„å…ˆå","è²©å£²æ—¥","YJã‚³ãƒ¼ãƒ‰","ãƒ¦ãƒ‹ãƒ¼ã‚¯ã‚³ãƒ¼ãƒ‰_YJ","å•†å“å","åˆè¨ˆé‡‘é¡","ç²—åˆ©"]
    miss = [c for c in required if c not in df.columns]
    if miss:
        st.error(f"[SNAPSHOT] å¿…é ˆåˆ—ä¸è¶³: {miss}")
        st.write("å–å¾—åˆ—:", list(df.columns))
        st.stop()

    df["å£²ä¸Šæ—¥"] = safe_parse_date_series(df["è²©å£²æ—¥"]).dt.date
    df = df[df["å£²ä¸Šæ—¥"].notna()].copy()

    df["å£²ä¸Šé¡"] = pd.to_numeric(df["åˆè¨ˆé‡‘é¡"], errors="coerce").fillna(0)
    df["åˆ©ç›Š"] = pd.to_numeric(df["ç²—åˆ©"], errors="coerce").fillna(0)

    df["å¹´åº¦"] = df["å£²ä¸Šæ—¥"].apply(fy_year)
    df["å£²ä¸Šæœˆã‚­ãƒ¼"] = pd.to_datetime(df["å£²ä¸Šæ—¥"]).dt.strftime("%Y-%m")

    df["å¾—æ„å…ˆã‚³ãƒ¼ãƒ‰"] = df["å¾—æ„å…ˆã‚³ãƒ¼ãƒ‰"].astype(str)
    df["YJã‚³ãƒ¼ãƒ‰"] = df["YJã‚³ãƒ¼ãƒ‰"].astype(str)
    df["ãƒ¦ãƒ‹ãƒ¼ã‚¯ã‚³ãƒ¼ãƒ‰_YJ"] = df["ãƒ¦ãƒ‹ãƒ¼ã‚¯ã‚³ãƒ¼ãƒ‰_YJ"].astype(str)

    df["åˆ©ç›Šç‡"] = df.apply(lambda r: (r["åˆ©ç›Š"] / r["å£²ä¸Šé¡"]) if r["å£²ä¸Šé¡"] else 0, axis=1)

    # ãªã„åˆ—ã¯åŸ‹ã‚ã¦äº’æ›
    for col in ["å•†å“ã‚³ãƒ¼ãƒ‰","åŒ…è£…å˜ä½","æ•°é‡","å˜ä¾¡","JANã‚³ãƒ¼ãƒ‰","ãƒ­ãƒƒãƒˆNo","ä½¿ç”¨æœŸé™"]:
        if col not in df.columns:
            df[col] = "" if col in ["å•†å“ã‚³ãƒ¼ãƒ‰","åŒ…è£…å˜ä½","JANã‚³ãƒ¼ãƒ‰","ãƒ­ãƒƒãƒˆNo","ä½¿ç”¨æœŸé™"] else 0

    return df


@st.cache_data(ttl=1800)
def load_incremental_this_month():
    # ã“ã¡ã‚‰ã‚‚ã‚¹ã‚­ãƒ¼ãƒã«ä¾å­˜ã—ãªã„ã‚ˆã†ã€å­˜åœ¨åˆ—ã ã‘ã‚’ä½¿ã†
    cols = get_table_columns(TABLE_INC)

    # GASã®MERGEã§ä½¿ã£ã¦ã„ãŸåˆ—å€™è£œ
    # ï¼ˆå­˜åœ¨ã—ãªã„å ´åˆã¯å¾Œæ®µã§missingæ¤œçŸ¥ã—ã¦åœæ­¢ï¼‰
    q = f"""
    SELECT
      CAST(`å¾—æ„å…ˆã‚³ãƒ¼ãƒ‰` AS STRING) AS å¾—æ„å…ˆã‚³ãƒ¼ãƒ‰,
      `å¾—æ„å…ˆå` AS å¾—æ„å…ˆå,
      CAST(`YJCode` AS STRING) AS YJã‚³ãƒ¼ãƒ‰,
      CAST(`JAN` AS STRING) AS JANã‚³ãƒ¼ãƒ‰,
      CAST(`å•†å“ã‚³ãƒ¼ãƒ‰` AS STRING) AS å•†å“ã‚³ãƒ¼ãƒ‰,
      `å•†å“åç§°` AS å•†å“å,
      `åŒ…è£…å˜ä½` AS åŒ…è£…å˜ä½,
      CAST(`è²©å£²æ—¥` AS DATE) AS å£²ä¸Šæ—¥,
      CAST(`è²©å£²æ•°é‡` AS FLOAT64) AS æ•°é‡,
      CAST(`åˆè¨ˆé‡‘é¡` AS FLOAT64) AS åˆè¨ˆé‡‘é¡,
      CAST(`ç²—åˆ©` AS FLOAT64) AS ç²—åˆ©
    FROM `{TABLE_INC}`
    WHERE CAST(`è²©å£²æ—¥` AS DATE) >= DATE_TRUNC(CURRENT_DATE('Asia/Tokyo'), MONTH)
    """

    # ã‚‚ã—åˆ—åãŒé•ã†å ´åˆã¯ã“ã“ã§BadRequestã«ãªã‚‹ã®ã§ã€SQLã¨ã‚¨ãƒ©ãƒ¼ãŒç”»é¢ã«å‡ºã‚‹
    df = run_query_df(q, "INCREMENTAL")

    required = ["å¾—æ„å…ˆã‚³ãƒ¼ãƒ‰","å¾—æ„å…ˆå","å£²ä¸Šæ—¥","YJã‚³ãƒ¼ãƒ‰","æ•°é‡","å•†å“å","åˆè¨ˆé‡‘é¡","ç²—åˆ©"]
    miss = [c for c in required if c not in df.columns]
    if miss:
        st.error(f"[INCREMENTAL] å¿…é ˆåˆ—ä¸è¶³: {miss}")
        st.write("å–å¾—åˆ—:", list(df.columns))
        st.stop()

    df["å£²ä¸Šæ—¥"] = pd.to_datetime(df["å£²ä¸Šæ—¥"], errors="coerce").dt.date
    df = df[df["å£²ä¸Šæ—¥"].notna()].copy()

    df["å£²ä¸Šé¡"] = pd.to_numeric(df["åˆè¨ˆé‡‘é¡"], errors="coerce").fillna(0)
    df["åˆ©ç›Š"] = pd.to_numeric(df["ç²—åˆ©"], errors="coerce").fillna(0)

    df["å¹´åº¦"] = df["å£²ä¸Šæ—¥"].apply(fy_year)
    df["å£²ä¸Šæœˆã‚­ãƒ¼"] = pd.to_datetime(df["å£²ä¸Šæ—¥"]).dt.strftime("%Y-%m")

    df["å¾—æ„å…ˆã‚³ãƒ¼ãƒ‰"] = df["å¾—æ„å…ˆã‚³ãƒ¼ãƒ‰"].astype(str)
    df["YJã‚³ãƒ¼ãƒ‰"] = df["YJã‚³ãƒ¼ãƒ‰"].astype(str)
    df["ãƒ¦ãƒ‹ãƒ¼ã‚¯ã‚³ãƒ¼ãƒ‰_YJ"] = df["å¾—æ„å…ˆã‚³ãƒ¼ãƒ‰"] + "_" + df["YJã‚³ãƒ¼ãƒ‰"]

    df["åˆ©ç›Šç‡"] = df.apply(lambda r: (r["åˆ©ç›Š"] / r["å£²ä¸Šé¡"]) if r["å£²ä¸Šé¡"] else 0, axis=1)
    return df


@st.cache_data(ttl=1800)
def load_sales_merged():
    today = datetime.now().date()
    m0 = month_start(today)

    snap = load_snapshot_2y()
    snap = snap[snap["å£²ä¸Šæ—¥"] < m0].copy()

    inc = load_incremental_this_month()

    merged = pd.concat([snap, inc], ignore_index=True)

    # å¿µã®ãŸã‚é‡è¤‡æ’é™¤
    subset = ["å¾—æ„å…ˆã‚³ãƒ¼ãƒ‰","å£²ä¸Šæ—¥","YJã‚³ãƒ¼ãƒ‰","å•†å“ã‚³ãƒ¼ãƒ‰","æ•°é‡","åˆè¨ˆé‡‘é¡"]
    subset = [c for c in subset if c in merged.columns]
    if subset:
        merged = merged.drop_duplicates(subset=subset, keep="last")

    return merged, len(snap), len(inc)

# =========================
# New delivery flag
# =========================
def add_new_delivery_flag_by_unique_yj(df: pd.DataFrame, lookback_days=365) -> pd.DataFrame:
    d = df.sort_values(["ãƒ¦ãƒ‹ãƒ¼ã‚¯ã‚³ãƒ¼ãƒ‰_YJ", "å£²ä¸Šæ—¥"]).copy()
    d["prev_date"] = d.groupby("ãƒ¦ãƒ‹ãƒ¼ã‚¯ã‚³ãƒ¼ãƒ‰_YJ")["å£²ä¸Šæ—¥"].shift(1)
    d["gap_days"] = (pd.to_datetime(d["å£²ä¸Šæ—¥"]) - pd.to_datetime(d["prev_date"])).dt.days
    d["is_new_delivery"] = d["prev_date"].isna() | (d["gap_days"] > lookback_days)
    return d

# =========================
# MAIN
# =========================
df_sales, n_snap, n_inc = load_sales_merged()
df_sales = add_new_delivery_flag_by_unique_yj(df_sales, LOOKBACK_DAYS_NEW)

st.title("Strategic Sales Console")
c0, c1, c2 = st.columns(3)
c0.metric("ã‚¹ãƒŠãƒƒãƒ—ï¼ˆå½“æœˆé™¤å¤–ï¼‰", f"{n_snap:,}")
c1.metric("å½“æœˆï¼ˆæ´—ã„æ›¿ãˆï¼‰", f"{n_inc:,}")
c2.metric("çµ±åˆå¾Œï¼ˆé‡è¤‡æ’é™¤ï¼‰", f"{len(df_sales):,}")

if df_sales.empty:
    st.error("ãƒ‡ãƒ¼ã‚¿ãŒç©ºã§ã™ã€‚")
    st.stop()

today = datetime.now().date()
yesterday = today - timedelta(days=1)
start_week = today - timedelta(days=today.weekday())
start_month = today.replace(day=1)
start_fy = fy_start(today)

fy0_start = start_fy
fy0_end = today
fy1_start = date(fy_year(today)-1, 4, 1)
fy1_end = same_day_last_year(today)

st.sidebar.title("ğŸ® ãƒ•ã‚£ãƒ«ã‚¿")
mode = st.sidebar.radio("ãƒ¢ãƒ¼ãƒ‰", ["ç®¡ç†è€…ãƒ¢ãƒ¼ãƒ‰", "å–¶æ¥­å“¡ãƒ¢ãƒ¼ãƒ‰"])
df_view = df_sales.copy()

if mode == "å–¶æ¥­å“¡ãƒ¢ãƒ¼ãƒ‰" and "æ‹…å½“ç¤¾å“¡å" in df_view.columns:
    staff = st.sidebar.selectbox("æ‹…å½“è€…", sorted(df_view["æ‹…å½“ç¤¾å“¡å"].dropna().unique()))
    df_view = df_view[df_view["æ‹…å½“ç¤¾å“¡å"] == staff]

# â‘  FY-to-date ranking
st.header("â‘  å¹´åº¦å†… å£²ä¸Šãƒ»åˆ©ç›Šãƒ»åˆ©ç›Šç‡ / æ˜¨å¹´æ¯”è¼ƒï¼ˆå¾—æ„å…ˆãƒ©ãƒ³ã‚­ãƒ³ã‚°ï¼‰")
df_fy0 = df_view[(df_view["å£²ä¸Šæ—¥"] >= fy0_start) & (df_view["å£²ä¸Šæ—¥"] <= fy0_end)].copy()
df_fy1 = df_view[(df_view["å£²ä¸Šæ—¥"] >= fy1_start) & (df_view["å£²ä¸Šæ—¥"] <= fy1_end)].copy()

a1, a2, a3, a4 = st.columns(4)
a1.metric("å£²ä¸Šï¼ˆä»Šå¹´åº¦å†…ï¼‰", yen(df_fy0["å£²ä¸Šé¡"].sum()))
a2.metric("åˆ©ç›Šï¼ˆä»Šå¹´åº¦å†…ï¼‰", yen(df_fy0["åˆ©ç›Š"].sum()))
a3.metric("åˆ©ç›Šç‡ï¼ˆä»Šå¹´åº¦å†…ï¼‰", f"{(df_fy0['åˆ©ç›Š'].sum()/df_fy0['å£²ä¸Šé¡'].sum()*100) if df_fy0['å£²ä¸Šé¡'].sum() else 0:.2f}%")
a4.metric("å£²ä¸Šå‰å¹´å·®", yen(df_fy0["å£²ä¸Šé¡"].sum() - df_fy1["å£²ä¸Šé¡"].sum()))

topn = st.slider("è¡¨ç¤ºä»¶æ•°", 10, 100, 30)

s0 = df_fy0.groupby("å¾—æ„å…ˆå", dropna=False).agg(å£²ä¸Š=("å£²ä¸Šé¡","sum"), åˆ©ç›Š=("åˆ©ç›Š","sum")).reset_index()
s1 = df_fy1.groupby("å¾—æ„å…ˆå", dropna=False).agg(å£²ä¸Š_æ˜¨å¹´=("å£²ä¸Šé¡","sum")).reset_index()
rank = s0.merge(s1, on="å¾—æ„å…ˆå", how="left").fillna(0)
rank["åˆ©ç›Šç‡"] = rank.apply(lambda r: (r["åˆ©ç›Š"]/r["å£²ä¸Š"]) if r["å£²ä¸Š"] else 0, axis=1)
rank["å‰å¹´å·®"] = rank["å£²ä¸Š"] - rank["å£²ä¸Š_æ˜¨å¹´"]
rank = rank.sort_values("å£²ä¸Š", ascending=False).head(topn)

st.dataframe(
    rank.style.format({"å£²ä¸Š":"Â¥{:,.0f}","åˆ©ç›Š":"Â¥{:,.0f}","åˆ©ç›Šç‡":"{:.2%}","å£²ä¸Š_æ˜¨å¹´":"Â¥{:,.0f}","å‰å¹´å·®":"Â¥{:,.0f}"}),
    use_container_width=True
)

# â‘¡ New delivery summary
st.divider()
st.header("â‘¡ æ–°è¦ç´å“ã‚µãƒãƒªãƒ¼ï¼ˆå¾—æ„å…ˆÃ—YJ / éå»1å¹´å£²ä¸Šãªã—ï¼‰")
period = st.radio("æœŸé–“", ["æ˜¨æ—¥","ä»Šé€±","ä»Šæœˆ","å¹´åº¦å†…"], horizontal=True)

if period == "æ˜¨æ—¥":
    base = df_view[df_view["å£²ä¸Šæ—¥"] == yesterday]
elif period == "ä»Šé€±":
    base = df_view[df_view["å£²ä¸Šæ—¥"] >= start_week]
elif period == "ä»Šæœˆ":
    base = df_view[df_view["å£²ä¸Šæ—¥"] >= start_month]
else:
    base = df_view[df_view["å£²ä¸Šæ—¥"] >= start_fy]

new_df = base[base["is_new_delivery"]].copy()

b1, b2, b3, b4 = st.columns(4)
b1.metric("è»’æ•°ï¼ˆå¾—æ„å…ˆæ•°ï¼‰", f"{new_df['å¾—æ„å…ˆã‚³ãƒ¼ãƒ‰'].nunique():,}")
b2.metric("é‡‘é¡ï¼ˆå£²ä¸Šï¼‰", yen(new_df["å£²ä¸Šé¡"].sum()))
b3.metric("å“ç›®æ•°ï¼ˆYJæ•°ï¼‰", f"{new_df['YJã‚³ãƒ¼ãƒ‰'].nunique():,}")
b4.metric("åˆ©ç›Šç‡", f"{(new_df['åˆ©ç›Š'].sum()/new_df['å£²ä¸Šé¡'].sum()*100) if new_df['å£²ä¸Šé¡'].sum() else 0:.2f}%")

# â‘¢ diff
st.divider()
st.header("â‘¢ ä¸‹é™ / ä¸Šæ˜‡ å¾—æ„å…ˆå·®é¡ãƒ©ãƒ³ã‚­ãƒ³ã‚°ï¼ˆå¹´åº¦å†… vs æ˜¨å¹´åŒæ—¥ã¾ã§ï¼‰")
c0_ = df_fy0.groupby("å¾—æ„å…ˆå", dropna=False)["å£²ä¸Šé¡"].sum().reset_index().rename(columns={"å£²ä¸Šé¡":"å£²ä¸Š_ä»Šå¹´"})
c1_ = df_fy1.groupby("å¾—æ„å…ˆå", dropna=False)["å£²ä¸Šé¡"].sum().reset_index().rename(columns={"å£²ä¸Šé¡":"å£²ä¸Š_æ˜¨å¹´"})
cd = c0_.merge(c1_, on="å¾—æ„å…ˆå", how="outer").fillna(0)
cd["å·®é¡"] = cd["å£²ä¸Š_ä»Šå¹´"] - cd["å£²ä¸Š_æ˜¨å¹´"]

tab_l, tab_g = st.tabs(["ğŸ”» ä¸‹è½", "ğŸ”¼ ä¸Šæ˜‡"])
with tab_l:
    st.dataframe(cd.sort_values("å·®é¡", ascending=True).head(30).style.format({"å£²ä¸Š_ä»Šå¹´":"Â¥{:,.0f}","å£²ä¸Š_æ˜¨å¹´":"Â¥{:,.0f}","å·®é¡":"Â¥{:,.0f}"}), use_container_width=True)
with tab_g:
    st.dataframe(cd.sort_values("å·®é¡", ascending=False).head(30).style.format({"å£²ä¸Š_ä»Šå¹´":"Â¥{:,.0f}","å£²ä¸Š_æ˜¨å¹´":"Â¥{:,.0f}","å·®é¡":"Â¥{:,.0f}"}), use_container_width=True)

st.caption("â€»BadRequestãŒå‡ºã‚‹å ´åˆã€ç”»é¢ã«SQLã¨ã‚¨ãƒ©ãƒ¼ã‚’è¡¨ç¤ºã—ã¾ã™ã€‚ã¾ãšãã‚Œã‚’è²¼ã£ã¦ãã ã•ã„ã€‚")
