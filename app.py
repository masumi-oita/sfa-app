# app.py
# SFA Sales OS (å…¥å£) - Admin (Org summary + Top/Bottom) + Drill + Perf Logs
# - Uses non-scoped views to avoid role/area gating (as requested: æœªåˆ†é¡žOKãƒ»å…¨å“¡çµ±æ‹¬OK)
# - Adds check mechanisms: SQL timing, timeout, cache control, query logging, parameterized queries
# - Designed to be pasted/replaced as-is in your Streamlit repo

import os
import time
from dataclasses import dataclass
from datetime import date, datetime
from typing import Any, Dict, List, Optional, Tuple

import pandas as pd
import streamlit as st

from google.cloud import bigquery
from google.oauth2 import service_account


# =========================
# CONFIG
# =========================
PROJECT_ID = "salesdb-479915"
DATASET_ID = "sales_data"
LOCATION = "asia-northeast1"

# Non-scoped (stable) views for Admin
VIEW_SYS_CURRENT_MONTH = f"`{PROJECT_ID}.{DATASET_ID}.v_sys_current_month`"
VIEW_ADMIN_ORG_FYTD = f"`{PROJECT_ID}.{DATASET_ID}.v_admin_org_fytd_summary`"
VIEW_ADMIN_TOP = f"`{PROJECT_ID}.{DATASET_ID}.v_admin_customer_fytd_top_named`"
VIEW_ADMIN_BOTTOM = f"`{PROJECT_ID}.{DATASET_ID}.v_admin_customer_fytd_bottom_named`"

# Drill views (you already have these)
VIEW_DRILL_CUST_ITEM_MONTH = f"`{PROJECT_ID}.{DATASET_ID}.v_sales_detail_by_customer_item_month`"
VIEW_DRILL_CUST_YJ_MONTH = f"`{PROJECT_ID}.{DATASET_ID}.v_sales_detail_by_customer_yj_month`"

DEFAULT_TIMEOUT_SEC = 60  # UI-level timeout target (BQ job can still run; we handle UX)
DEFAULT_LIMIT = 200

st.set_page_config(
    page_title="SFA Sales OSï¼ˆå…¥å£ï¼‰",
    page_icon="ðŸ“Š",
    layout="wide",
)

# =========================
# STATE / LOGGING
# =========================
if "query_logs" not in st.session_state:
    st.session_state.query_logs = []  # List[dict]
if "cache_buster" not in st.session_state:
    st.session_state.cache_buster = 0


@dataclass
class QueryResult:
    df: pd.DataFrame
    elapsed_s: float
    bytes_processed_gb: float
    rows: int
    job_id: Optional[str]
    sql: str
    ok: bool
    error: Optional[str] = None


def _now_ts() -> str:
    return datetime.now().strftime("%Y-%m-%d %H:%M:%S")


def log_query(name: str, res: QueryResult):
    st.session_state.query_logs.append(
        {
            "ts": _now_ts(),
            "name": name,
            "ok": res.ok,
            "elapsed_s": round(res.elapsed_s, 3),
            "bytes_gb": round(res.bytes_processed_gb, 3),
            "rows": int(res.rows),
            "job_id": res.job_id or "",
            "error": res.error or "",
            "sql": res.sql if len(res.sql) <= 4000 else (res.sql[:4000] + "\n-- (truncated)"),
        }
    )


# =========================
# AUTH / CLIENT
# =========================
@st.cache_resource(show_spinner=False)
def get_bq_client() -> bigquery.Client:
    """
    Uses st.secrets["gcp_service_account"] if present (recommended).
    """
    if "gcp_service_account" in st.secrets:
        creds = service_account.Credentials.from_service_account_info(st.secrets["gcp_service_account"])
        return bigquery.Client(project=PROJECT_ID, credentials=creds, location=LOCATION)
    # fallback: default credentials
    return bigquery.Client(project=PROJECT_ID, location=LOCATION)


def _use_bqstorage_api() -> bool:
    return bool(st.session_state.get("use_bqstorage", False))


def _show_sql() -> bool:
    return bool(st.session_state.get("show_sql", False))


def _enable_perf_log() -> bool:
    return bool(st.session_state.get("enable_perf_log", True))


def _cache_key_suffix() -> int:
    # increments when user hits "ã‚­ãƒ£ãƒƒã‚·ãƒ¥ç„¡åŠ¹åŒ–"
    return int(st.session_state.get("cache_buster", 0))


def _safe_float_gb(x: Optional[int]) -> float:
    if not x:
        return 0.0
    return float(x) / (1024**3)


def run_bq_query(
    name: str,
    sql: str,
    params: Optional[List[bigquery.ScalarQueryParameter]] = None,
    timeout_sec: int = DEFAULT_TIMEOUT_SEC,
    use_cache: bool = True,
) -> QueryResult:
    """
    Executes BigQuery SQL with optional query parameters (prevents illegal character / injection).
    Adds timing + bytes processed + job id.
    """
    client = get_bq_client()

    job_config = bigquery.QueryJobConfig(use_query_cache=use_cache)
    if params:
        job_config.query_parameters = params

    t0 = time.time()
    job_id = None
    try:
        job = client.query(sql, job_config=job_config)
        job_id = job.job_id

        # We don't hard-cancel BQ job; we use a UI timeout for responsiveness
        # but still try to fetch within timeout.
        df = job.result(timeout=timeout_sec).to_dataframe(
            create_bqstorage_client=_use_bqstorage_api()
        )
        elapsed = time.time() - t0

        bytes_gb = _safe_float_gb(getattr(job, "total_bytes_processed", None))
        rows = int(len(df))

        res = QueryResult(
            df=df,
            elapsed_s=elapsed,
            bytes_processed_gb=bytes_gb,
            rows=rows,
            job_id=job_id,
            sql=sql,
            ok=True,
            error=None,
        )
        if _enable_perf_log():
            log_query(name, res)
        return res

    except Exception as e:
        elapsed = time.time() - t0
        res = QueryResult(
            df=pd.DataFrame(),
            elapsed_s=elapsed,
            bytes_processed_gb=0.0,
            rows=0,
            job_id=job_id,
            sql=sql,
            ok=False,
            error=str(e),
        )
        if _enable_perf_log():
            log_query(name, res)
        return res


# Cache layer (data)
@st.cache_data(show_spinner=False, ttl=300)
def cached_query(
    cache_buster: int,
    name: str,
    sql: str,
    params_tuples: Tuple[Tuple[str, str, Any], ...],
    timeout_sec: int,
    use_cache: bool,
    use_bqstorage: bool,
) -> QueryResult:
    # Rebuild params objects inside cache function
    params: List[bigquery.ScalarQueryParameter] = []
    for ptype, pname, pval in params_tuples:
        params.append(bigquery.ScalarQueryParameter(pname, ptype, pval))
    # use_bqstorage is read from st.session_state normally, but passed here to bind cache key
    st.session_state["use_bqstorage"] = use_bqstorage
    return run_bq_query(name=name, sql=sql, params=params or None, timeout_sec=timeout_sec, use_cache=use_cache)


def query_df(
    name: str,
    sql: str,
    params: Optional[List[Tuple[str, str, Any]]] = None,
    timeout_sec: int = DEFAULT_TIMEOUT_SEC,
    use_cache: bool = True,
) -> QueryResult:
    """
    Wrapper that applies st.cache_data if enabled by UI.
    params: list of tuples (type, name, value) e.g. ("STRING","customer_code","123")
    """
    params = params or []
    params_tuples = tuple((t, n, v) for (t, n, v) in params)

    if st.session_state.get("disable_data_cache", False):
        # no cache
        bq_params = [bigquery.ScalarQueryParameter(n, t, v) for (t, n, v) in params]
        return run_bq_query(name, sql, bq_params or None, timeout_sec=timeout_sec, use_cache=use_cache)

    return cached_query(
        _cache_key_suffix(),
        name,
        sql,
        params_tuples,
        timeout_sec,
        use_cache,
        _use_bqstorage_api(),
    )


# =========================
# UI HELPERS
# =========================
def jp_col_rename(df: pd.DataFrame) -> pd.DataFrame:
    """
    Best-effort Japanese labels for common columns.
    If your views use different column names, they will still display as-is.
    """
    mapping = {
        "customer_code": "å¾—æ„å…ˆã‚³ãƒ¼ãƒ‰",
        "customer_name": "å¾—æ„å…ˆå",
        "branch_code": "æ”¯åº—ã‚³ãƒ¼ãƒ‰",
        "branch_name": "æ”¯åº—å",
        "staff_code": "æ‹…å½“è€…ã‚³ãƒ¼ãƒ‰",
        "staff_name": "æ‹…å½“è€…å",
        "sales_amount": "å£²ä¸Š",
        "gross_profit": "ç²—åˆ©",
        "gross_profit_rate": "ç²—åˆ©çŽ‡",
        "gp_rate": "ç²—åˆ©çŽ‡",
        "yoy_sales_amount": "å‰å¹´æ¯”ï¼ˆå£²ä¸Šï¼‰",
        "yoy_gross_profit": "å‰å¹´æ¯”ï¼ˆç²—åˆ©ï¼‰",
        "mom_sales_amount": "å‰æœˆå·®ï¼ˆå£²ä¸Šï¼‰",
        "mom_gross_profit": "å‰æœˆå·®ï¼ˆç²—åˆ©ï¼‰",
        "rank": "é †ä½",
        "fiscal_year": "å¹´åº¦",
        "fiscal_month": "æœˆ",
        "month": "æœˆ",
        "ym": "å¹´æœˆ",
        "item_name": "å“ç›®å",
        "item_code": "å“ç›®ã‚³ãƒ¼ãƒ‰",
        "yj_code": "YJã‚³ãƒ¼ãƒ‰",
        "jan": "JAN",
        "quantity": "æ•°é‡",
    }
    cols = {c: mapping.get(c, c) for c in df.columns}
    return df.rename(columns=cols)


def render_result_header(name: str, res: QueryResult):
    if res.ok:
        st.caption(f"[{name}] query={res.elapsed_s:.1f}s / bytes={res.bytes_processed_gb:.2f}GB / rows={res.rows}")
        if _show_sql():
            with st.expander(f"SQL: {name}", expanded=False):
                st.code(res.sql, language="sql")
    else:
        st.error(f"[{name}] ERROR: {res.error}")
        if _show_sql():
            with st.expander(f"SQL: {name}", expanded=True):
                st.code(res.sql, language="sql")


# =========================
# SIDEBAR (Login / Controls)
# =========================
st.sidebar.title("ãƒ­ã‚°ã‚¤ãƒ³")
user_email = st.sidebar.text_input("user_emailï¼ˆãƒ¡ãƒ¼ãƒ«ï¼‰", value=st.query_params.get("user_email", ""))

st.sidebar.markdown("---")
st.sidebar.toggle("é«˜é€Ÿè»¢é€ï¼ˆStorage APIï¼‰ã‚’è©¦ã™", value=False, key="use_bqstorage")
st.sidebar.toggle("ãƒã‚§ãƒƒã‚¯è¡¨ç¤ºï¼ˆSQLè¨ˆæ¸¬ã‚’è¡¨ç¤ºï¼‰", value=True, key="enable_perf_log")
st.sidebar.toggle("SQLã‚’è¡¨ç¤ºï¼ˆãƒ‡ãƒãƒƒã‚°ç”¨ï¼‰", value=False, key="show_sql")
st.sidebar.toggle("ãƒ‡ãƒ¼ã‚¿ã‚­ãƒ£ãƒƒã‚·ãƒ¥ç„¡åŠ¹åŒ–", value=False, key="disable_data_cache")

timeout_sec = st.sidebar.number_input("BQ timeoutï¼ˆç§’ï¼‰", min_value=10, max_value=300, value=DEFAULT_TIMEOUT_SEC, step=10)

c1, c2 = st.sidebar.columns(2)
if c1.button("è¨ˆæ¸¬ãƒ­ã‚°ã‚’ã‚¯ãƒªã‚¢"):
    st.session_state.query_logs = []
    st.toast("è¨ˆæ¸¬ãƒ­ã‚°ã‚’ã‚¯ãƒªã‚¢ã—ã¾ã—ãŸ")
if c2.button("ã‚­ãƒ£ãƒƒã‚·ãƒ¥ç„¡åŠ¹åŒ–"):
    st.session_state.cache_buster += 1
    st.toast("ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚­ãƒ¼ã‚’æ›´æ–°ã—ã¾ã—ãŸï¼ˆæ¬¡å›žã‹ã‚‰å†å–å¾—ï¼‰")

st.sidebar.markdown("---")
st.sidebar.caption("â€»é…ã„/å›ºã¾ã‚‹æ™‚ï¼š\n- Storage API ON\n- ãƒ‡ãƒ¼ã‚¿ã‚­ãƒ£ãƒƒã‚·ãƒ¥ç„¡åŠ¹åŒ– ON\n- SQLè¡¨ç¤º ON ã§åŽŸå› ç‰¹å®š")

# =========================
# HEADER / HEALTH
# =========================
st.title("SFA Sales OSï¼ˆå…¥å£ï¼‰")

# Current month (sys)
res_month = query_df(
    name="sys_current_month",
    sql=f"SELECT * FROM {VIEW_SYS_CURRENT_MONTH} LIMIT 1",
    timeout_sec=timeout_sec,
    use_cache=True,
)
colA, colB, colC, colD = st.columns([1, 2, 1, 1])
with colA:
    render_result_header("sys_current_month", res_month)
    if res_month.ok and not res_month.df.empty:
        current_month = str(res_month.df.iloc[0, 0])
        st.metric("Current month", current_month)
    else:
        st.metric("Current month", "â€”")

with colB:
    st.metric("ãƒ­ã‚°ã‚¤ãƒ³æ°å", user_email if user_email else "ï¼ˆæœªå…¥åŠ›ï¼‰")

# Role display: per request, treat everyone as HQ_ADMIN (çµ±æ‹¬). Still show what table says if available.
role_tier = "HQ_ADMIN"
area_name = "çµ±æ‹¬"
with colC:
    st.metric("role_tier", role_tier)
with colD:
    st.metric("area", area_name)

st.markdown("---")

# =========================
# MAIN TABS
# =========================
tab_admin, tab_drill, tab_logs = st.tabs(["ç®¡ç†è€…å…¥å£ï¼ˆåˆ†æžï¼‰", "ãƒ‰ãƒªãƒ«ï¼ˆæ˜Žç´°ï¼‰", "è¨ˆæ¸¬ãƒ­ã‚°ï¼ˆé…ã„åŽŸå› ï¼‰"])


# =========================
# ADMIN: FYTD summary + Top/Bottom
# =========================
with tab_admin:
    st.subheader("A) å¹´åº¦ç´¯è¨ˆï¼ˆFYTDï¼‰")

    res_org = query_df(
        name="admin_org_fytd_summary",
        sql=f"SELECT * FROM {VIEW_ADMIN_ORG_FYTD}",
        timeout_sec=timeout_sec,
        use_cache=True,
    )
    render_result_header("admin_org_fytd_summary", res_org)

    if res_org.ok and not res_org.df.empty:
        df_org = jp_col_rename(res_org.df.copy())

        # Show KPI-like metrics if columns exist
        # We do best-effort; if not found, show the table.
        cols = [c.lower() for c in res_org.df.columns]
        # find likely columns
        def pick(*cands: str) -> Optional[str]:
            for x in cands:
                if x in cols:
                    return res_org.df.columns[cols.index(x)]
            return None

        sales_col = pick("sales_amount", "sales", "sales_total", "amount")
        gp_col = pick("gross_profit", "gp", "profit")
        gpr_col = pick("gross_profit_rate", "gp_rate", "profit_rate")

        m1, m2, m3 = st.columns(3)
        if sales_col:
            m1.metric("å£²ä¸Šï¼ˆFYTDï¼‰", f"{res_org.df[sales_col].fillna(0).sum():,.0f}")
        else:
            m1.metric("å£²ä¸Šï¼ˆFYTDï¼‰", "â€”")
        if gp_col:
            m2.metric("ç²—åˆ©ï¼ˆFYTDï¼‰", f"{res_org.df[gp_col].fillna(0).sum():,.0f}")
        else:
            m2.metric("ç²—åˆ©ï¼ˆFYTDï¼‰", "â€”")
        if gpr_col:
            try:
                v = float(res_org.df[gpr_col].dropna().iloc[0])
                m3.metric("ç²—åˆ©çŽ‡ï¼ˆFYTDï¼‰", f"{v*100:.1f}%")
            except Exception:
                m3.metric("ç²—åˆ©çŽ‡ï¼ˆFYTDï¼‰", "â€”")
        else:
            m3.metric("ç²—åˆ©çŽ‡ï¼ˆFYTDï¼‰", "â€”")

        st.dataframe(df_org, use_container_width=True, height=220)
    else:
        st.warning("FYTDã‚µãƒžãƒªãƒ¼ãŒå–å¾—ã§ãã¾ã›ã‚“ï¼ˆVIEWã‚’ç¢ºèªï¼‰")

    st.markdown("---")
    st.subheader("B) FYTD MoMï¼ˆå‰æœˆå·®ï¼‰ãƒ©ãƒ³ã‚­ãƒ³ã‚°ï¼ˆå¾—æ„å…ˆï¼‰")

    topN = st.slider("è¡¨ç¤ºä»¶æ•°", min_value=10, max_value=200, value=50, step=10)

    cL, cR = st.columns(2)

    with cL:
        st.markdown("### ðŸ“‰ ä¸‹è½ï¼ˆFYTD å‰æœˆå·®ï¼‰")
        res_bottom = query_df(
            name="admin_customer_fytd_bottom_named",
            sql=f"SELECT * FROM {VIEW_ADMIN_BOTTOM} LIMIT @lim",
            params=[("INT64", "lim", int(topN))],
            timeout_sec=timeout_sec,
            use_cache=True,
        )
        render_result_header("admin_customer_fytd_bottom_named", res_bottom)

        if res_bottom.ok and not res_bottom.df.empty:
            df_b = jp_col_rename(res_bottom.df.copy())
            st.dataframe(df_b, use_container_width=True, height=420)
        else:
            st.info("ä¸‹è½ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")

    with cR:
        st.markdown("### ðŸ“ˆ ä¼¸é•·ï¼ˆFYTD å‰æœˆå·®ï¼‰")
        res_top = query_df(
            name="admin_customer_fytd_top_named",
            sql=f"SELECT * FROM {VIEW_ADMIN_TOP} LIMIT @lim",
            params=[("INT64", "lim", int(topN))],
            timeout_sec=timeout_sec,
            use_cache=True,
        )
        render_result_header("admin_customer_fytd_top_named", res_top)

        if res_top.ok and not res_top.df.empty:
            df_t = jp_col_rename(res_top.df.copy())
            st.dataframe(df_t, use_container_width=True, height=420)
        else:
            st.info("ä¼¸é•·ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")

    st.caption("â€»ã“ã“ã¯â€œç®¡ç†è€…ï¼ˆçµ±æ‹¬ï¼‰â€å‰æã§ã€scoped VIEW ã‚’é€šã•ãšã«å®‰å®šç¨¼åƒã•ã›ã¦ã„ã¾ã™ã€‚")


# =========================
# DRILL: customer x item/yj month
# =========================
with tab_drill:
    st.subheader("ãƒ‰ãƒªãƒ«ï¼ˆå¾—æ„å…ˆ â†’ æœˆæ¬¡ â†’ å“ç›®/YJï¼‰")

    st.caption("â€»ã“ã“ã¯å¿…ãšãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿SQLã§å®Ÿè¡Œã—ã¾ã™ï¼ˆIllegal input character å¯¾ç­–ï¼‰")

    # customer picker from top/bottom if available
    cust_candidates: List[Tuple[str, str]] = []  # (code, name)
    for res in [locals().get("res_top"), locals().get("res_bottom")]:
        if isinstance(res, QueryResult) and res.ok and not res.df.empty:
            cols = [c.lower() for c in res.df.columns]
            if "customer_code" in cols and "customer_name" in cols:
                ccode = res.df[res.df.columns[cols.index("customer_code")]].astype(str)
                cname = res.df[res.df.columns[cols.index("customer_name")]].astype(str)
                cust_candidates += list(zip(ccode.tolist(), cname.tolist()))
    cust_candidates = list(dict.fromkeys(cust_candidates))  # dedup preserve order

    left, right = st.columns([2, 1])
    with left:
        if cust_candidates:
            label_map = {f"{n}ï¼ˆ{c}ï¼‰": (c, n) for c, n in cust_candidates}
            pick_label = st.selectbox("å¾—æ„å…ˆï¼ˆãƒ©ãƒ³ã‚­ãƒ³ã‚°ã‹ã‚‰é¸æŠžï¼‰", options=list(label_map.keys()))
            customer_code, customer_name = label_map[pick_label]
        else:
            customer_code = st.text_input("å¾—æ„å…ˆã‚³ãƒ¼ãƒ‰ï¼ˆç›´æŽ¥å…¥åŠ›ï¼‰", value="")
            customer_name = st.text_input("å¾—æ„å…ˆåï¼ˆä»»æ„ï¼‰", value="")
    with right:
        drill_mode = st.radio("ãƒ‰ãƒªãƒ«è»¸", options=["å¾—æ„å…ˆÃ—å“ç›®ï¼ˆæœˆæ¬¡ï¼‰", "å¾—æ„å…ˆÃ—YJï¼ˆæœˆæ¬¡ï¼‰"], horizontal=False)

    # period controls (month start/end as DATE)
    p1, p2, p3 = st.columns([1, 1, 2])
    with p1:
        start_date = st.date_input("é–‹å§‹æ—¥", value=date(2025, 4, 1))
    with p2:
        end_date = st.date_input("çµ‚äº†æ—¥", value=date.today())
    with p3:
        limit = st.number_input("æœ€å¤§è¡Œæ•°", min_value=50, max_value=5000, value=DEFAULT_LIMIT, step=50)

    run = st.button("ãƒ‰ãƒªãƒ«å®Ÿè¡Œ", type="primary", disabled=not bool(customer_code))

    if run and customer_code:
        if drill_mode == "å¾—æ„å…ˆÃ—å“ç›®ï¼ˆæœˆæ¬¡ï¼‰":
            sql = f"""
            SELECT *
            FROM {VIEW_DRILL_CUST_ITEM_MONTH}
            WHERE customer_code = @customer_code
              AND sales_month >= @start_date
              AND sales_month <= @end_date
            ORDER BY sales_month DESC
            LIMIT @lim
            """
        else:
            sql = f"""
            SELECT *
            FROM {VIEW_DRILL_CUST_YJ_MONTH}
            WHERE customer_code = @customer_code
              AND sales_month >= @start_date
              AND sales_month <= @end_date
            ORDER BY sales_month DESC
            LIMIT @lim
            """

        res_drill = query_df(
            name="drill",
            sql=sql,
            params=[
                ("STRING", "customer_code", str(customer_code)),
                ("DATE", "start_date", start_date),
                ("DATE", "end_date", end_date),
                ("INT64", "lim", int(limit)),
            ],
            timeout_sec=timeout_sec,
            use_cache=True,
        )
        render_result_header("drill", res_drill)
        if res_drill.ok:
            st.dataframe(jp_col_rename(res_drill.df), use_container_width=True, height=520)
        else:
            st.error("ãƒ‰ãƒªãƒ«å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸã€‚ä¸Šã®ã‚¨ãƒ©ãƒ¼ã¨SQLã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")


# =========================
# PERF LOGS
# =========================
with tab_logs:
    st.subheader("è¨ˆæ¸¬ãƒ­ã‚°ï¼ˆã©ã®SQLãŒé…ã„ã‹ãƒ»å¤±æ•—ã—ãŸã‹ï¼‰")
    logs = st.session_state.query_logs
    if not logs:
        st.info("ã¾ã ãƒ­ã‚°ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚ç®¡ç†è€…å…¥å£/ãƒ‰ãƒªãƒ«ã‚’å®Ÿè¡Œã™ã‚‹ã¨è¨˜éŒ²ã•ã‚Œã¾ã™ã€‚")
    else:
        df_log = pd.DataFrame(logs)
        st.dataframe(df_log, use_container_width=True, height=420)

        # quick summary
        st.markdown("#### ç›´è¿‘ã®å‚¾å‘")
        ok_rate = (df_log["ok"].sum() / len(df_log)) * 100
        st.write(f"- æˆåŠŸçŽ‡: {ok_rate:.1f}%")
        st.write(f"- æœ€å¤§æ™‚é–“: {df_log['elapsed_s'].max():.2f}s")
        st.write(f"- æœ€å¤§bytes: {df_log['bytes_gb'].max():.2f}GB")

        if st.button("ãƒ­ã‚°CSVãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ç”¨ã«è¡¨ç¤ºï¼ˆã‚³ãƒ”ãƒ¼ï¼‰"):
            st.code(df_log.to_csv(index=False), language="text")
