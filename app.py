# app.py
# SFA Sales OS (å…¥å£) - Admin + Drill + Perf Logs
# FIX-1: Do NOT mutate st.session_state inside cached funcs
# FIX-2: st.cache_data return value must be serializable -> return (df, meta_dict)

import time
from datetime import date, datetime
from typing import Any, Dict, List, Optional, Tuple

import pandas as pd
import streamlit as st

from google.cloud import bigquery
from google.oauth2 import service_account


# =========================
# CONFIG
# =========================
PROJECT_ID = "salesdb-479915"
DATASET_ID = "sales_data"
LOCATION = "asia-northeast1"

VIEW_SYS_CURRENT_MONTH = f"`{PROJECT_ID}.{DATASET_ID}.v_sys_current_month`"
VIEW_ADMIN_ORG_FYTD = f"`{PROJECT_ID}.{DATASET_ID}.v_admin_org_fytd_summary`"
VIEW_ADMIN_TOP = f"`{PROJECT_ID}.{DATASET_ID}.v_admin_customer_fytd_top_named`"
VIEW_ADMIN_BOTTOM = f"`{PROJECT_ID}.{DATASET_ID}.v_admin_customer_fytd_bottom_named`"

VIEW_DRILL_CUST_ITEM_MONTH = f"`{PROJECT_ID}.{DATASET_ID}.v_sales_detail_by_customer_item_month`"
VIEW_DRILL_CUST_YJ_MONTH = f"`{PROJECT_ID}.{DATASET_ID}.v_sales_detail_by_customer_yj_month`"

DEFAULT_TIMEOUT_SEC = 60
DEFAULT_LIMIT = 200

st.set_page_config(page_title="SFA Sales OSï¼ˆå…¥å£ï¼‰", page_icon="ðŸ“Š", layout="wide")


# =========================
# STATE
# =========================
if "query_logs" not in st.session_state:
    st.session_state.query_logs = []
if "cache_buster" not in st.session_state:
    st.session_state.cache_buster = 0


def _now_ts() -> str:
    return datetime.now().strftime("%Y-%m-%d %H:%M:%S")


def _safe_float_gb(x: Optional[int]) -> float:
    if not x:
        return 0.0
    return float(x) / (1024**3)


def log_query(name: str, ok: bool, elapsed_s: float, bytes_gb: float, rows: int, job_id: str, error: str, sql: str):
    st.session_state.query_logs.append(
        {
            "ts": _now_ts(),
            "name": name,
            "ok": ok,
            "elapsed_s": round(float(elapsed_s), 3),
            "bytes_gb": round(float(bytes_gb), 3),
            "rows": int(rows),
            "job_id": job_id or "",
            "error": error or "",
            "sql": sql if len(sql) <= 4000 else (sql[:4000] + "\n-- (truncated)"),
        }
    )


def perf_enabled() -> bool:
    return bool(st.session_state.get("enable_perf_log", True))


def show_sql() -> bool:
    return bool(st.session_state.get("show_sql", False))


# =========================
# AUTH / CLIENT
# =========================
@st.cache_resource(show_spinner=False)
def get_bq_client() -> bigquery.Client:
    if "gcp_service_account" in st.secrets:
        creds = service_account.Credentials.from_service_account_info(st.secrets["gcp_service_account"])
        return bigquery.Client(project=PROJECT_ID, credentials=creds, location=LOCATION)
    return bigquery.Client(project=PROJECT_ID, location=LOCATION)


def run_bq_query_df(
    name: str,
    sql: str,
    params: Optional[List[bigquery.ScalarQueryParameter]] = None,
    timeout_sec: int = DEFAULT_TIMEOUT_SEC,
    use_cache: bool = True,
    use_bqstorage: bool = False,
) -> Tuple[pd.DataFrame, Dict[str, Any]]:
    """
    Returns (df, meta_dict) where meta_dict is JSON-serializable.
    """
    client = get_bq_client()
    job_config = bigquery.QueryJobConfig(use_query_cache=use_cache)
    if params:
        job_config.query_parameters = params

    t0 = time.time()
    job_id = ""
    try:
        job = client.query(sql, job_config=job_config)
        job_id = job.job_id

        df = job.result(timeout=timeout_sec).to_dataframe(create_bqstorage_client=use_bqstorage)

        elapsed = time.time() - t0
        bytes_gb = _safe_float_gb(getattr(job, "total_bytes_processed", None))
        rows = int(len(df))

        meta = {
            "ok": True,
            "elapsed_s": float(elapsed),
            "bytes_gb": float(bytes_gb),
            "rows": int(rows),
            "job_id": job_id,
            "error": "",
            "sql": sql,
        }

        # NOTE: logging is outside cache function, but run_bq_query_df can be used non-cached too
        return df, meta

    except Exception as e:
        elapsed = time.time() - t0
        meta = {
            "ok": False,
            "elapsed_s": float(elapsed),
            "bytes_gb": 0.0,
            "rows": 0,
            "job_id": job_id,
            "error": str(e),
            "sql": sql,
        }
        return pd.DataFrame(), meta


# =========================
# CACHE LAYER
# =========================
@st.cache_data(show_spinner=False, ttl=300)
def cached_query_df(
    cache_buster: int,
    name: str,
    sql: str,
    params_tuples: Tuple[Tuple[str, str, Any], ...],
    timeout_sec: int,
    use_cache: bool,
    use_bqstorage: bool,
) -> Tuple[pd.DataFrame, Dict[str, Any]]:
    """
    Cached function must return serializable values -> (DataFrame, dict)
    """
    params: List[bigquery.ScalarQueryParameter] = []
    for ptype, pname, pval in params_tuples:
        params.append(bigquery.ScalarQueryParameter(pname, ptype, pval))

    df, meta = run_bq_query_df(
        name=name,
        sql=sql,
        params=params or None,
        timeout_sec=timeout_sec,
        use_cache=use_cache,
        use_bqstorage=use_bqstorage,
    )
    # meta is dict, serializable
    return df, meta


def query_df(
    name: str,
    sql: str,
    params: Optional[List[Tuple[str, str, Any]]] = None,
    timeout_sec: int = DEFAULT_TIMEOUT_SEC,
    use_cache: bool = True,
) -> Tuple[pd.DataFrame, Dict[str, Any]]:
    params = params or []
    params_tuples = tuple((t, n, v) for (t, n, v) in params)

    use_bqstorage = bool(st.session_state.get("use_bqstorage", False))
    disable_cache = bool(st.session_state.get("disable_data_cache", False))
    cache_buster = int(st.session_state.get("cache_buster", 0))

    if disable_cache:
        bq_params = [bigquery.ScalarQueryParameter(n, t, v) for (t, n, v) in params]
        df, meta = run_bq_query_df(
            name=name,
            sql=sql,
            params=bq_params or None,
            timeout_sec=timeout_sec,
            use_cache=use_cache,
            use_bqstorage=use_bqstorage,
        )
    else:
        df, meta = cached_query_df(
            cache_buster,
            name,
            sql,
            params_tuples,
            timeout_sec,
            use_cache,
            use_bqstorage,
        )

    # logging here (outside cache) is safe
    if perf_enabled():
        log_query(
            name=name,
            ok=bool(meta.get("ok")),
            elapsed_s=float(meta.get("elapsed_s", 0.0)),
            bytes_gb=float(meta.get("bytes_gb", 0.0)),
            rows=int(meta.get("rows", 0)),
            job_id=str(meta.get("job_id", "")),
            error=str(meta.get("error", "")),
            sql=str(meta.get("sql", "")),
        )

    return df, meta


# =========================
# UI HELPERS
# =========================
def jp_col_rename(df: pd.DataFrame) -> pd.DataFrame:
    mapping = {
        "customer_code": "å¾—æ„å…ˆã‚³ãƒ¼ãƒ‰",
        "customer_name": "å¾—æ„å…ˆå",
        "branch_code": "æ”¯åº—ã‚³ãƒ¼ãƒ‰",
        "branch_name": "æ”¯åº—å",
        "staff_code": "æ‹…å½“è€…ã‚³ãƒ¼ãƒ‰",
        "staff_name": "æ‹…å½“è€…å",
        "sales_amount": "å£²ä¸Š",
        "gross_profit": "ç²—åˆ©",
        "gross_profit_rate": "ç²—åˆ©çŽ‡",
        "gp_rate": "ç²—åˆ©çŽ‡",
        "yoy_sales_amount": "å‰å¹´æ¯”ï¼ˆå£²ä¸Šï¼‰",
        "yoy_gross_profit": "å‰å¹´æ¯”ï¼ˆç²—åˆ©ï¼‰",
        "mom_sales_amount": "å‰æœˆå·®ï¼ˆå£²ä¸Šï¼‰",
        "mom_gross_profit": "å‰æœˆå·®ï¼ˆç²—åˆ©ï¼‰",
        "rank": "é †ä½",
        "fiscal_year": "å¹´åº¦",
        "fiscal_month": "æœˆ",
        "month": "æœˆ",
        "ym": "å¹´æœˆ",
        "item_name": "å“ç›®å",
        "item_code": "å“ç›®ã‚³ãƒ¼ãƒ‰",
        "yj_code": "YJã‚³ãƒ¼ãƒ‰",
        "jan": "JAN",
        "quantity": "æ•°é‡",
        "sales_month": "å£²ä¸Šæœˆ",
    }
    return df.rename(columns={c: mapping.get(c, c) for c in df.columns})


def render_result_header(name: str, meta: Dict[str, Any]):
    if meta.get("ok"):
        st.caption(
            f"[{name}] query={float(meta.get('elapsed_s', 0.0)):.1f}s / "
            f"bytes={float(meta.get('bytes_gb', 0.0)):.2f}GB / rows={int(meta.get('rows', 0))}"
        )
        if show_sql():
            with st.expander(f"SQL: {name}", expanded=False):
                st.code(str(meta.get("sql", "")), language="sql")
    else:
        st.error(f"[{name}] ERROR: {meta.get('error')}")
        if show_sql():
            with st.expander(f"SQL: {name}", expanded=True):
                st.code(str(meta.get("sql", "")), language="sql")


# =========================
# SIDEBAR
# =========================
st.sidebar.title("ãƒ­ã‚°ã‚¤ãƒ³")
user_email = st.sidebar.text_input("user_emailï¼ˆãƒ¡ãƒ¼ãƒ«ï¼‰", value=st.query_params.get("user_email", ""))

st.sidebar.markdown("---")
st.sidebar.toggle("é«˜é€Ÿè»¢é€ï¼ˆStorage APIï¼‰ã‚’è©¦ã™", value=False, key="use_bqstorage")
st.sidebar.toggle("ãƒã‚§ãƒƒã‚¯è¡¨ç¤ºï¼ˆSQLè¨ˆæ¸¬ã‚’è¡¨ç¤ºï¼‰", value=True, key="enable_perf_log")
st.sidebar.toggle("SQLã‚’è¡¨ç¤ºï¼ˆãƒ‡ãƒãƒƒã‚°ç”¨ï¼‰", value=False, key="show_sql")
st.sidebar.toggle("ãƒ‡ãƒ¼ã‚¿ã‚­ãƒ£ãƒƒã‚·ãƒ¥ç„¡åŠ¹åŒ–", value=False, key="disable_data_cache")

timeout_sec = st.sidebar.number_input("BQ timeoutï¼ˆç§’ï¼‰", min_value=10, max_value=300, value=DEFAULT_TIMEOUT_SEC, step=10)

c1, c2 = st.sidebar.columns(2)
if c1.button("è¨ˆæ¸¬ãƒ­ã‚°ã‚’ã‚¯ãƒªã‚¢"):
    st.session_state.query_logs = []
    st.toast("è¨ˆæ¸¬ãƒ­ã‚°ã‚’ã‚¯ãƒªã‚¢ã—ã¾ã—ãŸ")
if c2.button("ã‚­ãƒ£ãƒƒã‚·ãƒ¥ç„¡åŠ¹åŒ–ï¼ˆå†å–å¾—ï¼‰"):
    st.session_state.cache_buster += 1
    st.toast("ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚­ãƒ¼ã‚’æ›´æ–°ã—ã¾ã—ãŸï¼ˆæ¬¡å›žã‹ã‚‰å†å–å¾—ï¼‰")

st.sidebar.markdown("---")
st.sidebar.caption("â€»é…ã„/å›ºã¾ã‚‹æ™‚ï¼šStorage API ON / ã‚­ãƒ£ãƒƒã‚·ãƒ¥ç„¡åŠ¹åŒ– ON / SQLè¡¨ç¤º ON ã§åŽŸå› ç‰¹å®š")


# =========================
# HEADER
# =========================
st.title("SFA Sales OSï¼ˆå…¥å£ï¼‰")

df_month, meta_month = query_df(
    name="sys_current_month",
    sql=f"SELECT * FROM {VIEW_SYS_CURRENT_MONTH} LIMIT 1",
    timeout_sec=timeout_sec,
    use_cache=True,
)

colA, colB, colC, colD = st.columns([1, 2, 1, 1])
with colA:
    render_result_header("sys_current_month", meta_month)
    if meta_month.get("ok") and not df_month.empty:
        current_month = str(df_month.iloc[0, 0])
        st.metric("Current month", current_month)
    else:
        st.metric("Current month", "â€”")

with colB:
    st.metric("ãƒ­ã‚°ã‚¤ãƒ³æ°å", user_email if user_email else "ï¼ˆæœªå…¥åŠ›ï¼‰")

# æ–¹é‡: æœªåˆ†é¡žãƒ»å…¨å“¡çµ±æ‹¬ã§ã‚ˆã„ï¼ˆè¡¨ç¤ºä¸Šã®å›ºå®šï¼‰
with colC:
    st.metric("role_tier", "HQ_ADMIN")
with colD:
    st.metric("area", "çµ±æ‹¬")

st.markdown("---")

tab_admin, tab_drill, tab_logs = st.tabs(["ç®¡ç†è€…å…¥å£ï¼ˆåˆ†æžï¼‰", "ãƒ‰ãƒªãƒ«ï¼ˆæ˜Žç´°ï¼‰", "è¨ˆæ¸¬ãƒ­ã‚°ï¼ˆé…ã„åŽŸå› ï¼‰"])


# =========================
# ADMIN
# =========================
with tab_admin:
    st.subheader("A) å¹´åº¦ç´¯è¨ˆï¼ˆFYTDï¼‰")

    df_org, meta_org = query_df(
        name="admin_org_fytd_summary",
        sql=f"SELECT * FROM {VIEW_ADMIN_ORG_FYTD}",
        timeout_sec=timeout_sec,
        use_cache=True,
    )
    render_result_header("admin_org_fytd_summary", meta_org)

    if meta_org.get("ok") and not df_org.empty:
        df_org_jp = jp_col_rename(df_org.copy())

        cols_lower = [c.lower() for c in df_org.columns]

        def pick(*cands: str) -> Optional[str]:
            for x in cands:
                if x in cols_lower:
                    return df_org.columns[cols_lower.index(x)]
            return None

        sales_col = pick("sales_amount", "sales", "sales_total", "amount")
        gp_col = pick("gross_profit", "gp", "profit")
        gpr_col = pick("gross_profit_rate", "gp_rate", "profit_rate")

        m1, m2, m3 = st.columns(3)
        m1.metric("å£²ä¸Šï¼ˆFYTDï¼‰", f"{df_org[sales_col].fillna(0).sum():,.0f}" if sales_col else "â€”")
        m2.metric("ç²—åˆ©ï¼ˆFYTDï¼‰", f"{df_org[gp_col].fillna(0).sum():,.0f}" if gp_col else "â€”")
        if gpr_col:
            try:
                v = float(df_org[gpr_col].dropna().iloc[0])
                m3.metric("ç²—åˆ©çŽ‡ï¼ˆFYTDï¼‰", f"{v*100:.1f}%")
            except Exception:
                m3.metric("ç²—åˆ©çŽ‡ï¼ˆFYTDï¼‰", "â€”")
        else:
            m3.metric("ç²—åˆ©çŽ‡ï¼ˆFYTDï¼‰", "â€”")

        st.dataframe(df_org_jp, use_container_width=True, height=220)
    else:
        st.warning("FYTDã‚µãƒžãƒªãƒ¼ãŒå–å¾—ã§ãã¾ã›ã‚“ï¼ˆVIEWã‚’ç¢ºèªï¼‰")

    st.markdown("---")
    st.subheader("B) FYTD MoMï¼ˆå‰æœˆå·®ï¼‰ãƒ©ãƒ³ã‚­ãƒ³ã‚°ï¼ˆå¾—æ„å…ˆï¼‰")

    topN = st.slider("è¡¨ç¤ºä»¶æ•°", min_value=10, max_value=200, value=50, step=10)

    cL, cR = st.columns(2)

    with cL:
        st.markdown("### ðŸ“‰ ä¸‹è½ï¼ˆFYTD å‰æœˆå·®ï¼‰")
        df_bottom, meta_bottom = query_df(
            name="admin_customer_fytd_bottom_named",
            sql=f"SELECT * FROM {VIEW_ADMIN_BOTTOM} LIMIT @lim",
            params=[("INT64", "lim", int(topN))],
            timeout_sec=timeout_sec,
            use_cache=True,
        )
        render_result_header("admin_customer_fytd_bottom_named", meta_bottom)
        if meta_bottom.get("ok") and not df_bottom.empty:
            st.dataframe(jp_col_rename(df_bottom.copy()), use_container_width=True, height=420)
        else:
            st.info("ä¸‹è½ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")

    with cR:
        st.markdown("### ðŸ“ˆ ä¼¸é•·ï¼ˆFYTD å‰æœˆå·®ï¼‰")
        df_top, meta_top = query_df(
            name="admin_customer_fytd_top_named",
            sql=f"SELECT * FROM {VIEW_ADMIN_TOP} LIMIT @lim",
            params=[("INT64", "lim", int(topN))],
            timeout_sec=timeout_sec,
            use_cache=True,
        )
        render_result_header("admin_customer_fytd_top_named", meta_top)
        if meta_top.get("ok") and not df_top.empty:
            st.dataframe(jp_col_rename(df_top.copy()), use_container_width=True, height=420)
        else:
            st.info("ä¼¸é•·ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")

    st.caption("â€»ã“ã“ã¯â€œå…¨å“¡çµ±æ‹¬â€å‰æã§ã€scoped VIEW ã‚’é€šã•ãšã«å®‰å®šç¨¼åƒã•ã›ã¦ã„ã¾ã™ã€‚")


# =========================
# DRILL
# =========================
with tab_drill:
    st.subheader("ãƒ‰ãƒªãƒ«ï¼ˆå¾—æ„å…ˆ â†’ æœˆæ¬¡ â†’ å“ç›®/YJï¼‰")
    st.caption("â€»ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿SQLã§å®Ÿè¡Œï¼ˆIllegal input character å¯¾ç­–ï¼‰")

    cust_candidates: List[Tuple[str, str]] = []
    for df in [locals().get("df_top"), locals().get("df_bottom")]:
        if isinstance(df, pd.DataFrame) and not df.empty:
            cols = [c.lower() for c in df.columns]
            if "customer_code" in cols and "customer_name" in cols:
                ccode = df[df.columns[cols.index("customer_code")]].astype(str)
                cname = df[df.columns[cols.index("customer_name")]].astype(str)
                cust_candidates += list(zip(ccode.tolist(), cname.tolist()))
    cust_candidates = list(dict.fromkeys(cust_candidates))

    left, right = st.columns([2, 1])
    with left:
        if cust_candidates:
            label_map = {f"{n}ï¼ˆ{c}ï¼‰": (c, n) for c, n in cust_candidates}
            pick_label = st.selectbox("å¾—æ„å…ˆï¼ˆãƒ©ãƒ³ã‚­ãƒ³ã‚°ã‹ã‚‰é¸æŠžï¼‰", options=list(label_map.keys()))
            customer_code, customer_name = label_map[pick_label]
        else:
            customer_code = st.text_input("å¾—æ„å…ˆã‚³ãƒ¼ãƒ‰ï¼ˆç›´æŽ¥å…¥åŠ›ï¼‰", value="")
            customer_name = st.text_input("å¾—æ„å…ˆåï¼ˆä»»æ„ï¼‰", value="")
    with right:
        drill_mode = st.radio("ãƒ‰ãƒªãƒ«è»¸", options=["å¾—æ„å…ˆÃ—å“ç›®ï¼ˆæœˆæ¬¡ï¼‰", "å¾—æ„å…ˆÃ—YJï¼ˆæœˆæ¬¡ï¼‰"], horizontal=False)

    p1, p2, p3 = st.columns([1, 1, 2])
    with p1:
        start_date = st.date_input("é–‹å§‹æ—¥", value=date(2025, 4, 1))
    with p2:
        end_date = st.date_input("çµ‚äº†æ—¥", value=date.today())
    with p3:
        limit = st.number_input("æœ€å¤§è¡Œæ•°", min_value=50, max_value=5000, value=DEFAULT_LIMIT, step=50)

    run = st.button("ãƒ‰ãƒªãƒ«å®Ÿè¡Œ", type="primary", disabled=not bool(customer_code))

    if run and customer_code:
        if drill_mode == "å¾—æ„å…ˆÃ—å“ç›®ï¼ˆæœˆæ¬¡ï¼‰":
            sql = f"""
            SELECT *
            FROM {VIEW_DRILL_CUST_ITEM_MONTH}
            WHERE customer_code = @customer_code
              AND sales_month >= @start_date
              AND sales_month <= @end_date
            ORDER BY sales_month DESC
            LIMIT @lim
            """
        else:
            sql = f"""
            SELECT *
            FROM {VIEW_DRILL_CUST_YJ_MONTH}
            WHERE customer_code = @customer_code
              AND sales_month >= @start_date
              AND sales_month <= @end_date
            ORDER BY sales_month DESC
            LIMIT @lim
            """

        df_drill, meta_drill = query_df(
            name="drill",
            sql=sql,
            params=[
                ("STRING", "customer_code", str(customer_code)),
                ("DATE", "start_date", start_date),
                ("DATE", "end_date", end_date),
                ("INT64", "lim", int(limit)),
            ],
            timeout_sec=timeout_sec,
            use_cache=True,
        )
        render_result_header("drill", meta_drill)
        if meta_drill.get("ok"):
            st.dataframe(jp_col_rename(df_drill.copy()), use_container_width=True, height=520)
        else:
            st.error("ãƒ‰ãƒªãƒ«å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸã€‚ä¸Šã®ã‚¨ãƒ©ãƒ¼ã¨SQLã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")


# =========================
# PERF LOGS
# =========================
with tab_logs:
    st.subheader("è¨ˆæ¸¬ãƒ­ã‚°ï¼ˆã©ã®SQLãŒé…ã„ã‹ãƒ»å¤±æ•—ã—ãŸã‹ï¼‰")
    logs = st.session_state.query_logs
    if not logs:
        st.info("ã¾ã ãƒ­ã‚°ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚ç®¡ç†è€…å…¥å£/ãƒ‰ãƒªãƒ«ã‚’å®Ÿè¡Œã™ã‚‹ã¨è¨˜éŒ²ã•ã‚Œã¾ã™ã€‚")
    else:
        df_log = pd.DataFrame(logs)
        st.dataframe(df_log, use_container_width=True, height=420)

        st.markdown("#### ç›´è¿‘ã®å‚¾å‘")
        ok_rate = (df_log["ok"].sum() / len(df_log)) * 100
        st.write(f"- æˆåŠŸçŽ‡: {ok_rate:.1f}%")
        st.write(f"- æœ€å¤§æ™‚é–“: {df_log['elapsed_s'].max():.2f}s")
        st.write(f"- æœ€å¤§bytes: {df_log['bytes_gb'].max():.2f}GB")

        if st.button("ãƒ­ã‚°CSVï¼ˆã‚³ãƒ”ãƒ¼ç”¨ï¼‰"):
            st.code(df_log.to_csv(index=False), language="text")
